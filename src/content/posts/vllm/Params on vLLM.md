---
title: "Params on vLLM"
published: 2026-01-27
description: "Params on vLLM"
image: ""
tags: ["vllm","Params on vLLM"]
category: vllm
draft: false
lang: ""
---

# `EngineArgs` å’Œ `SamplingParams` çš„åŒºåˆ«

EngineArgs configures the runtime engine, including model loading, GPU parallelism, and memory management.
SamplingParams configures decoding behavior, including randomness, length control, stopping criteria, and constraints.â†³

 vLLM å½“æˆä¸€ä¸ªâ€œå†™ä½œå·¥åŽ‚â€ï¼š

-   **EngineArgs**ï¼šé€‰åŽ‚æˆ¿ã€é€‰æœºå™¨ã€æŽ¥å¤šå°‘ç”µã€é…å¤šå°‘å·¥äºº
     â†’ å†³å®šäº§èƒ½å’Œèƒ½ä¸èƒ½ç”Ÿäº§
-   **SamplingParams**ï¼šå†³å®šå†™ä½œé£Žæ ¼ã€å†™å‡ ç¯‡ã€å†™å¤šé•¿ã€ä»€ä¹ˆæ—¶å€™åœ
     â†’ å†³å®šå…·ä½“äº§å‡ºçš„å†…å®¹

EngineArgsï¼šåˆå§‹åŒ–ç”¨ä¸€æ¬¡

```
llm = LLM(
    model="facebook/opt-125m",
    tensor_parallel_size=1,
    dtype="float16",
    gpu_memory_utilization=0.9
)
```

SamplingParamsï¼šæ¯æ¬¡ç”Ÿæˆå¯å˜

```
params = SamplingParams(temperature=0.7, top_p=0.9, max_tokens=128)
out = llm.generate(["Hello my name is"], params)
```

## **EngineArgs = Engine / Runtime Configuration**

å®ƒä¸»è¦ç®¡è¿™äº›äº‹æƒ…ï¼š

-   **åŠ è½½ä»€ä¹ˆæ¨¡åž‹**ï¼š`model`, `tokenizer`, `revision`
-   **æ€Žä¹ˆç”¨ GPU è·‘**ï¼š`tensor_parallel_size`, `dtype`, `quantization`
-   **æ˜¾å­˜æ€Žä¹ˆåˆ†é…**ï¼š`gpu_memory_utilization`, `swap_space`, `cpu_offload_gb`
-   **æ€§èƒ½/å…¼å®¹å¼€å…³**ï¼š`enforce_eager`, `compilation_config`

ðŸ“Œ ç‰¹ç‚¹ï¼š

-   ä¸€èˆ¬åœ¨ **åˆå§‹åŒ– LLM/Engine çš„æ—¶å€™è®¾ç½®ä¸€æ¬¡**
-   å½±å“ **æ€§èƒ½ã€å¹¶è¡Œã€æ˜¾å­˜ã€èƒ½ä¸èƒ½åŠ è½½æˆåŠŸ**



## **SamplingParams = Decoding / Generation Configuration**

å®ƒç®¡çš„æ˜¯â€œè¾“å‡ºæ–‡æœ¬æ€Žä¹ˆç”Ÿæˆâ€ï¼š

-   **éšæœºç¨‹åº¦**ï¼š`temperature`, `top_p`, `top_k`
-   **è¾“å‡ºé•¿åº¦**ï¼š`max_tokens`, `min_tokens`
-   **åœæ­¢è§„åˆ™**ï¼š`stop`, `stop_token_ids`, `ignore_eos`
-   **ç”Ÿæˆå‡ æ¡**ï¼š`n`, `best_of`
-   **é˜²é‡å¤**ï¼š`repetition_penalty`, `presence_penalty`â†³
-   **é«˜çº§çº¦æŸ**ï¼š`logit_bias`, `guided_decoding`, `allowed_token_ids`

ðŸ“Œ ç‰¹ç‚¹ï¼š

-   é€šå¸¸åœ¨ **æ¯æ¬¡ generate() çš„æ—¶å€™ä¼ å…¥**
-   å½±å“ **å†…å®¹é£Žæ ¼ã€é•¿åº¦ã€ç¨³å®šæ€§ã€æ ¼å¼çº¦æŸ**







# SamplingParams

## â‘  n

-   **Meaning**: It specifies how many candidate outputs to return in one generation.
-   **Default**: `1`
-   **Example**: `n=3` â†’ returns 3 different continuations for the same prompt.

------

## â‘¡ best_of

-   **Meaning**: Generate `best_of` candidates first, then choose the best ones to return (like â€œgenerate more, pick the bestâ€).
-   **Default**: `None`
-   **Requirement**: `best_of >= n`
-   **Use case**: Better quality, but slower and more expensive.

------

## â‘¢ _real_n (internal)

-   **Meaning**: When `best_of` is set, this stores the original `n` requested by the user.
-   **Default**: `None`
-   **Note**: You typically donâ€™t need to set this manually.

```python
# ç”¨æˆ·åˆ›å»º SamplingParams
params = SamplingParams(n=3, best_of=10)

# åˆå§‹åŒ–åŽçš„çŠ¶æ€ï¼š
# params.n = 10          # è¢«æ”¹ä¸º best_of çš„å€¼ï¼ˆç”¨äºŽç”Ÿæˆ 10 ä¸ªå€™é€‰ï¼‰
# params._real_n = 3      # ä¿å­˜åŽŸå§‹çš„ n å€¼ï¼ˆç”¨äºŽè¿”å›žæœ€å¥½çš„ 3 ä¸ªï¼‰
# params.best_of = 10
```

------

## â‘£ presence_penalty

-   ==**Meaning**: Penalizes tokens that already appeared â†’ encourages introducing new words/topics.==
-   **Default**: `0.0`
-   **Range**: `[-2, 2]`
-   **Effect**: Higher values make outputs more diverse and less repetitive.

------

## â‘¤ frequency_penalty

-   **Meaning**: Penalizes tokens more heavily the more often they appear â†’ reduces repeated phrases/loops.
-   **Default**: `0.0`
-   **Range**: `[-2, 2]`
-   **Effect**: Higher values reduce repetitive looping even more strongly.

------

## â‘¥ repetition_penalty

-   **Meaning**: A repetition penalty (common in Hugging Face style decoding) to discourage repeated tokens.
-   **Default**: `1.0`
-   **Range**: `(0, 2]`
-   **Typical values**: `1.05 ~ 1.2`; too high may make output weird.

------

## â‘¦ temperature

**temperatureï¼šå†³å®šâ€œéšæœºç¨‹åº¦â€**

**top_kï¼šå†³å®šâ€œæœ€å¤šå…è®¸å¤šå°‘ä¸ªå€™é€‰è¯â€**

**top_pï¼šå†³å®šâ€œåªå…è®¸æœ€é è°±çš„ä¸€æ‰¹å€™é€‰è¯â€**

**min_pï¼šå†³å®šâ€œå¤ªå°æ¦‚çŽ‡çš„ä¸€å¾‹ä¸å‡†é€‰â€**

**seedï¼šå†³å®šâ€œéšæœºç»“æžœèƒ½ä¸èƒ½å¤çŽ°â€**

==**As `temperature` increases, the output becomes more random.**==

-   **Meaning**: Sampling temperature that ==controls randomness.==
-   **Default**: `1.0`
-   **Range**: `>= 0`
-   **Effect**:
    -   `0` â†’ greedy smapling (always choose the highest-probability token)
    -   `0.2` â†’ very stable
    -   `0.8` â†’ commonly used
    -   `1.0` â†’ original distribution â†’ logits unchanged
    -   `1.2+` â†’ more random / creative but less reliable

------

## â‘§ top_p

-   **Meaning**: Nucleus sampling; sample only from tokens whose cumulative probability reaches `top_p`.
-   **Default**: `1.0`
-   **Range**: `(0, 1]`
-   **Effect**:
    -   `0.9 ~ 0.95` â†’ common settings
    -   Smaller = safer/more conservative, larger = more diverse

------

## â‘¨ top_k

-   **Meaning**: Sample only from the top K most likely tokens.
-   **Default**: `-1` (disabled)
-   **Valid values**: `-1` or `>=1` (cannot be 0)
-   **Effect**:
    -   `top_k=50` is common
    -   Often used together with `top_p` for stability

------

## â‘© min_p

-   **Meaning**: Filters out tokens with too small a probability (below a threshold).
-   **Default**: `0.0`
-   **Range**: `[0, 1]`
-   **Effect**: Higher values produce â€œcleanerâ€ outputs but reduce diversity.

------

## â‘ª seed

-   **Meaning**: Random seed (makes the output reproducible).
-   **Default**: `None`
-   **Special case**: If you pass `-1`, it becomes `None`.
-   **Use case**: Debugging, comparing parameter changes, reproducibility.

------

## â‘« stop

-   **Meaning**: Stop strings; generation stops when any stop string appears.
-   **Default**: `None` (will become `[]`)
-   **Supported**: a string or a list of strings
-   **Note**: Cannot include an empty string `""`.

------

## â‘¬ stop_token_ids

-   **Meaning**: Stop token IDs; generation stops when any of these tokens appears.
-   **Default**: `None` (will become `[]`)
-   **Use case**: Lower-level and more precise stopping than stop strings.

------

## â‘­ ignore_eos

-   **Meaning**: Whether to ignore the model EOS (end-of-sequence) token.
-   **Default**: `False`
-   **Effect**:
    -   `False` â†’ EOS stops generation
    -   `True` â†’ EOS does not stop (more likely to run until max_tokens)

------

## â‘® max_tokens

-   **Meaning**: Maximum number of tokens to generate (output length limit).
-   **Default**: `16`
-   **Requirement**: If not `None`, must be `>=1`
-   **Suggestion**: Usually set it higher, e.g. `max_tokens=128`.

------

## â‘¯ min_tokens

-   **Meaning**: Minimum number of tokens that must be generated (cannot stop before this).
-   **Default**: `0`
-   **Requirement**: `>=0` and `<= max_tokens`
-   **Use case**: Prevents the model from stopping too early.

------

## â‘° logprobs

-   **Meaning**: Returns top-k log probabilities for each generated token.
-   **Default**: `None`
-   **Special case**: If set to `True`, it becomes `1`.
-   **Use case**: Scoring, debugging, analyzing confidence.

------

## â‘± prompt_logprobs

-   **Meaning**: Returns log probabilities for prompt tokens.
-   **Default**: `None`
-   **Special case**: If set to `True`, it becomes `1`.
-   **Use case**: Analyze how the model â€œunderstandsâ€ the input prompt.

------

## â‘² detokenize

-   **Meaning**: Whether to convert tokens back into text output.
-   **Default**: `True`
-   **Note**: If `detokenize=False`, stop strings (`stop`) cannot be used.

------

## â‘³ skip_special_tokens

-   **Meaning**: Skip special tokens during decoding (like `<s>`, `</s>`).
-   **Default**: `True`
-   **Effect**: Cleaner final output.

------

## ã‰‘ spaces_between_special_tokens

-   **Meaning**: Whether to insert spaces between special tokens.
-   **Default**: `True`
-   **Usually**: No need to change.

------

## ã‰’ logits_processors

-   **Meaning**: Custom logits processor list (can directly modify token probabilities before sampling).
-   **Default**: `None`
-   **Use case**: Advanced controls like forcing formats, banning tokens, custom decoding logic.

------

## ã‰“ include_stop_str_in_output

-   **Meaning**: Whether to keep the stop string itself in the output.
-   **Default**: `False`
-   **Example**: stop="###"
    -   False â†’ stops before "###", output does not include it
    -   True â†’ output ends with "###"

------

## ã‰” truncate_prompt_tokens

-   **Meaning**: If prompt is too long, keep only the last N tokens.
-   **Default**: `None`
-   **Requirement**: Must be `>=1` if set.
-   **Use case**: Control context window and avoid OOM.

------

## ã‰• output_kind

-   **Meaning**: Output format style.
-   **Default**: `CUMULATIVE`
-   **Common values**:
    -   `CUMULATIVE` â†’ returns the full cumulative text each time
    -   `DELTA` â†’ returns only the newly generated chunk (streaming-friendly)
-   **Constraint**: With `DELTA`, `best_of` must equal `n`.

------

## Internal Fields (not required as user inputs)

## ã‰– output_text_buffer_length (internal)

-   **Meaning**: Buffers a few characters at the end to correctly detect stop strings.
-   **When used**: Automatically computed when stop strings are enabled and stop strings should not be included in output.

------

## ã‰— _all_stop_token_ids (internal)

-   **Meaning**: A combined set of all stop token IDs, including stop_token_ids and eos_token_id.
-   **Use case**: Used internally to decide when generation should stop.

------

## ã‰˜ guided_decoding

-   **Meaning**: Guided decoding parameters (e.g., force output to follow JSON/grammar constraints).
-   **Default**: `None`
-   **Use case**: Very useful when you need strict structured output.

------

## ã‰™ logit_bias

-   **Meaning**: Adds bias to specific token IDs (make them more or less likely).
-   **Default**: `None`
-   **Range**: Clamped to `[-100, 100]`
-   **Example**: `{token_id: +5}` makes a token more likely; `-100` almost bans it.

------

## ã‰š allowed_token_ids

-   **Meaning**: A whitelist of tokens that the model is allowed to generate.
-   **Default**: `None`
-   **Use case**: Very strict constraint (e.g., only allow digits).

------

## ã‰› extra_args

-   **Meaning**: Extra arguments dictionary (reserved for extensions).
-   **Default**: `None`

------

## ã‰œ bad_words

-   **Meaning**: A list of banned words that the model is not allowed to generate.
-   **Default**: `None` (will become `[]`)
-   **Use case**: Prevent certain words from appearing (closer to filtering than stopping).

------

## ã‰ _bad_words_token_ids (internal)

-   **Meaning**: Token-ID sequences converted from `bad_words`.
-   **Default**: `None`
-   **Filled by**: `update_from_tokenizer()` automatically.



---

âœ… **Chat / General**

```python
SamplingParams(temperature=0.7, top_p=0.9, max_tokens=256)
```

âœ… **More Stable (Less Random)**

```python
SamplingParams(temperature=0.2, top_p=0.9, max_tokens=256)
```

âœ… **Reduce Repetition**

```python
SamplingParams(temperature=0.7, top_p=0.9, repetition_penalty=1.1, max_tokens=256)
```

------



# EngineArgs



## â‘  `model`

-   **What it does**: Specifies which model checkpoint to load (HF name or local path).

```python
EngineArgs(model="facebook/opt-125m")
```

-   Local path:

```python
EngineArgs(model="/data/models/llama-7b")
```

------

## â‘¡ `task`

-   **What it does**: Specifies the task type (`auto` is common).

```python
EngineArgs(model="facebook/opt-125m", task="auto")
```

------

## â‘¢ `tokenizer`

-   **What it does**: Specifies which tokenizer to use.

```python
EngineArgs(
    model="facebook/opt-125m",
    tokenizer="facebook/opt-125m"
)
```

------

## â‘£ `tokenizer_mode`

-   **What it does**: Controls whether vLLM uses fast or slow tokenizer.

```python
EngineArgs(model="facebook/opt-125m", tokenizer_mode="auto")
```

-   Force slow tokenizer:

```python
EngineArgs(model="facebook/opt-125m", tokenizer_mode="slow")
```

------

## â‘¤ `skip_tokenizer_init`

-   **What it does**: Skips tokenizer initialization (useful when providing token IDs directly).

```python
EngineArgs(model="facebook/opt-125m", skip_tokenizer_init=True)
```

------

## â‘¥ `trust_remote_code`

-   **What it does**: Allows loading models that require custom HF code.

```python
EngineArgs(model="some-org/custom-model", trust_remote_code=True)
```

------

## â‘¦ `allowed_local_media_path`

-   **What it does**: Allows the engine to read local images/videos (multimodal use).

```python
EngineArgs(
    model="some-mm-model",
    allowed_local_media_path="/data/media"
)
```

------

## â‘§ `tensor_parallel_size`

-   **What it does**: Number of GPUs used for tensor parallelism.
    Single GPU:

```python
EngineArgs(model="facebook/opt-125m", tensor_parallel_size=1)
```

-   4 GPUs:

```python
EngineArgs(model="meta-llama/Llama-2-13b-hf", tensor_parallel_size=4)
```

------

## â‘¨ `dtype`

-   **What it does**: Precision of model weights/activations.
    Auto dtype:

```python
EngineArgs(model="facebook/opt-125m", dtype="auto")
```

-   Force FP16:

```python
EngineArgs(model="facebook/opt-125m", dtype="float16")
```

-   Force BF16:

```python
EngineArgs(model="facebook/opt-125m", dtype="bfloat16")
```

------

## â‘© `quantization`

-   **What it does**: Enables quantized inference (AWQ/GPTQ/FP8).
    AWQ:

```python
EngineArgs(model="some-awq-model", quantization="awq")
```

-   GPTQ:

```python
EngineArgs(model="some-gptq-model", quantization="gptq")
```

-   FP8 (experimental):

```python
EngineArgs(model="some-fp8-model", quantization="fp8")
```

------

## â‘ª `revision`

-   **What it does**: Pins a specific model version (branch/tag/commit).

```python
EngineArgs(
    model="facebook/opt-125m",
    revision="main"
)
```

-   Commit hash:

```python
EngineArgs(
    model="facebook/opt-125m",
    revision="a1b2c3d4e5f6"
)
```

------

## â‘« `tokenizer_revision`

-   **What it does**: Pins a specific tokenizer version.

```python
EngineArgs(
    model="facebook/opt-125m",
    tokenizer_revision="main"
)
```

------

## â‘¬ `seed`

-   **What it does**: Sets a random seed for reproducible sampling.

```python
EngineArgs(model="facebook/opt-125m", seed=42)
```

------

## â‘­ `gpu_memory_utilization`

-   **What it does**: Fraction of GPU memory reserved for weights + KV cache.
    Conservative:

```python
EngineArgs(model="facebook/opt-125m", gpu_memory_utilization=0.6)
```

-   Aggressive (higher throughput, higher OOM risk):

```python
EngineArgs(model="facebook/opt-125m", gpu_memory_utilization=0.95)
```

------

## â‘® `swap_space`

-   **What it does**: CPU swap memory per GPU (GiB), useful when `best_of > 1`.
    Disable swap (only if you always use best_of=1):

```python
EngineArgs(model="facebook/opt-125m", swap_space=0)
```

-   Enable swap:

```python
EngineArgs(model="facebook/opt-125m", swap_space=8)
```

------

## â‘¯ `cpu_offload_gb`

-   **What it does**: Offloads part of model weights to CPU RAM (GiB).

```python
EngineArgs(model="meta-llama/Llama-2-13b-hf", cpu_offload_gb=10)
```

------

## â‘° `enforce_eager`

-   **What it does**: Forces eager execution (disables CUDA graph optimizations).

```python
EngineArgs(model="facebook/opt-125m", enforce_eager=True)
```

------

## â‘± `max_seq_len_to_capture`

-   **What it does**: Max sequence length covered by CUDA graphs.
    Smaller (less graph coverage):

```python
EngineArgs(model="facebook/opt-125m", max_seq_len_to_capture=2048)
```

-   Larger (covers longer contexts):

```python
EngineArgs(model="facebook/opt-125m", max_seq_len_to_capture=8192)
```

------

## â‘² `disable_custom_all_reduce`

-   **What it does**: Disables vLLMâ€™s custom all-reduce implementation.

```python
EngineArgs(
    model="meta-llama/Llama-2-13b-hf",
    tensor_parallel_size=2,
    disable_custom_all_reduce=True
)
```

------

## â‘³ `disable_async_output_proc`

-   **What it does**: Disables async output processing (may reduce performance).

```python
EngineArgs(model="facebook/opt-125m", disable_async_output_proc=True)
```

------

## ã‰‘ `hf_overrides`

-   **What it does**: Overrides Hugging Face config values.
    Dict override:

```python
EngineArgs(
    model="facebook/opt-125m",
    hf_overrides={"max_position_embeddings": 4096}
)
```

------

## ã‰’ `mm_processor_kwargs`

-   **What it does**: Extra kwargs for multimodal processing.

```python
EngineArgs(
    model="some-mm-model",
    mm_processor_kwargs={"do_resize": True, "size": 512}
)
```

------

## ã‰“ `override_pooler_config`

-   **What it does**: Overrides pooler config (mostly for embedding tasks).

```python
EngineArgs(
    model="sentence-transformers/all-MiniLM-L6-v2",
    override_pooler_config={"pooling_type": "mean"}
)
```

------

## ã‰” `compilation_config`

-   **What it does**: Controls compilation optimization level or detailed config.
    Simple level:

```python
EngineArgs(model="facebook/opt-125m", compilation_config=2)
```

-   Full dict config:

```python
EngineArgs(
    model="facebook/opt-125m",
    compilation_config={"level": 2, "enable_cuda_graph": True}
)
```

------

## ã‰• `**kwargs`

-   **What it does**: Extra engine-level settings passed through to vLLM.
    Example (passing a custom worker class):

```python
EngineArgs(
    model="facebook/opt-125m",
    worker_cls="vllm.worker.worker.Worker"
)
```

------





